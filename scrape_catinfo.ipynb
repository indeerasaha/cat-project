{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cat Adoption Project \n",
    "\n",
    "Web scraping through catrangers' database to scrape information about adopted and up for adoption cats to combine into a csv file \n",
    "\n",
    "*change vars later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing modules \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops through all the cats available for adoption and adds them to a dictionary \n",
    "def for_adoption():\n",
    "    '''\n",
    "    Ret: dictionary with the keys: ids values: links \n",
    "    '''\n",
    "    master_url = 'https://www.catrangers.org/animals/list?Status=Available'\n",
    "    response = requests.get(master_url)\n",
    "    adoption_content = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    extracted_list = adoption_content.find_all('td', class_='portalTableValue')\n",
    "\n",
    "    id_dict = {}\n",
    "\n",
    "    for item in extracted_list:\n",
    "        link = item.find('a', href=True)\n",
    "        if link:\n",
    "            url = link['href']\n",
    "            animal_id = int(url.split('=')[-1])\n",
    "            id_dict[animal_id] = url\n",
    "\n",
    "    return id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function that uses the id_dict to write information into the csv file \n",
    "def animal_info(id_dict, start, end, mode_type='w'):\n",
    "    '''\n",
    "    params: \n",
    "    id_dict (type: dict)\n",
    "    start (type: int)\n",
    "    end (type: int)\n",
    "    mode_type (csv mode types)\n",
    "\n",
    "    purpose: writes into the cat information csv file \n",
    "    '''\n",
    "\n",
    "    filename = 'cat_information.csv'\n",
    "\n",
    "\n",
    "    try:\n",
    "        with open(filename, mode=mode_type, newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            \n",
    "            # Write headers\n",
    "            headers = [\n",
    "                \"Name\", \"Breed\", \"Sex and Status\", \"Age\", \"Size\", \n",
    "                \"Adoption Status\", \"Species\", \"Rescue ID\", \"General Color\", \n",
    "                \"Current Age\", \"Fence Required\", \"Declawed\", \"Housetrained\", \n",
    "                \"Exercise Needs\", \"Grooming Needs\", \"Shedding Amount\", \n",
    "                \"Owner Experience Needed\", \"Reaction to New People\"]\n",
    "            \n",
    "            writer.writerow(headers)\n",
    "\n",
    "            count = start\n",
    "            created_list = list(id_dict.values())\n",
    "\n",
    "            for value in created_list[start:end]:\n",
    "                entry = []\n",
    "                id_url = 'https://www.catrangers.org' + value\n",
    "\n",
    "                try:\n",
    "                    response = requests.get(id_url, timeout=10)\n",
    "                    response.raise_for_status()  \n",
    "                    # Raises an HTTPError for bad responses\n",
    "                except requests.RequestException as e:\n",
    "                    print(f\"Failed to retrieve data for {id_url}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                name = soup.find('span', class_='pageCenterTitle')\n",
    "                more_info = soup.find('p', style='text-align:center;')\n",
    "                info = more_info.find('strong') if more_info else None\n",
    "\n",
    "                entry.append(name.get_text(strip=True)[:-11])\n",
    "\n",
    "                if info:\n",
    "                    info_text = info.get_text(strip=True)\n",
    "                    text = info_text.replace('\\xa0', '').split('::')\n",
    "                  \n",
    "                                    \n",
    "                    entry.extend(text)\n",
    "                \n",
    "\n",
    "                elements = soup.find_all('li')\n",
    "                \n",
    "                for item in elements:\n",
    "                    item_text = item.get_text(strip=True)\n",
    "            \n",
    "                    \n",
    "                    entry.append(item_text)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                writer.writerow(entry)\n",
    "\n",
    "                print('Line:', count)\n",
    "                count += 1\n",
    "\n",
    "            print('Finished')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print('File does not exist')\n",
    "    except Exception as e:\n",
    "        print('An error has occurred:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_adopted(url=None):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "    driver.get('https://www.catrangers.org/animals/successes?')  # Replace with your URL\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    max_pages = 8\n",
    "    current_page = 0\n",
    "\n",
    "    dicty = {}\n",
    "\n",
    "    try:\n",
    "        while current_page < max_pages:\n",
    "            current_page += 1\n",
    "\n",
    "            try:\n",
    "                next_page_link = wait.until(EC.presence_of_element_located((By.LINK_TEXT, 'Next Page Â»')))\n",
    "\n",
    "                master_url = next_page_link.get_attribute('href')\n",
    "                response = requests.get(master_url)\n",
    "                adopted_content = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                extracted_list = adopted_content.find_all('table', align='center')\n",
    "\n",
    "                for table in extracted_list:\n",
    "                    a_tags = table.find_all('a', href=True)\n",
    "                    for a in a_tags:\n",
    "                        href = a['href']\n",
    "                        animal_id = int(href.split('=')[-1])\n",
    "                        dicty[animal_id] = href\n",
    "\n",
    "                next_page_link.click()\n",
    "\n",
    "                print('Clicked successfully', current_page)\n",
    "\n",
    "            except StaleElementReferenceException:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return dicty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicked successfully 1\n",
      "Clicked successfully 2\n",
      "Clicked successfully 3\n",
      "Clicked successfully 4\n",
      "Clicked successfully 5\n",
      "Clicked successfully 6\n",
      "Clicked successfully 7\n",
      "Clicked successfully 8\n"
     ]
    }
   ],
   "source": [
    "adopted_dict = scrub_adopted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line: 0\n",
      "Line: 1\n",
      "Line: 2\n",
      "Line: 3\n",
      "Line: 4\n",
      "Line: 5\n",
      "Line: 6\n",
      "Line: 7\n",
      "Line: 8\n",
      "Line: 9\n",
      "Line: 10\n",
      "Line: 11\n",
      "Line: 12\n",
      "Line: 13\n",
      "Line: 14\n",
      "Line: 15\n",
      "Line: 16\n",
      "Line: 17\n",
      "Line: 18\n",
      "Line: 19\n",
      "Line: 20\n",
      "Line: 21\n",
      "Line: 22\n",
      "Line: 23\n",
      "Line: 24\n",
      "Line: 25\n",
      "Line: 26\n",
      "Line: 27\n",
      "Line: 28\n",
      "Line: 29\n",
      "Line: 30\n",
      "Line: 31\n",
      "Line: 32\n",
      "Line: 33\n",
      "Line: 34\n",
      "Line: 35\n",
      "Line: 36\n",
      "Line: 37\n",
      "Line: 38\n",
      "Line: 39\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "animal_info(adopted_dict, 0, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adopted_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m animal_info(adopted_dict, \u001b[38;5;241m2000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(adopted_dict)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adopted_dict' is not defined"
     ]
    }
   ],
   "source": [
    "animal_info(adopted_dict, 2000, len(adopted_dict)+1, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "adoption_dict = for_adoption()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adoption_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line: 0\n",
      "Line: 1\n",
      "Line: 2\n",
      "Line: 3\n",
      "Line: 4\n",
      "Line: 5\n",
      "Line: 6\n",
      "Line: 7\n",
      "Line: 8\n",
      "Line: 9\n",
      "Line: 10\n",
      "Line: 11\n",
      "Line: 12\n",
      "Line: 13\n",
      "Line: 14\n",
      "Line: 15\n",
      "Line: 16\n",
      "Line: 17\n",
      "Line: 18\n",
      "Line: 19\n",
      "Line: 20\n",
      "Line: 21\n",
      "Line: 22\n",
      "Line: 23\n",
      "Line: 24\n",
      "Line: 25\n",
      "Line: 26\n",
      "Line: 27\n",
      "Line: 28\n",
      "Line: 29\n",
      "Line: 30\n",
      "Line: 31\n",
      "Line: 32\n",
      "Line: 33\n",
      "Line: 34\n",
      "Line: 35\n",
      "Line: 36\n",
      "Line: 37\n",
      "Line: 38\n",
      "Line: 39\n",
      "Line: 40\n",
      "Line: 41\n",
      "Line: 42\n",
      "Line: 43\n",
      "Line: 44\n",
      "Line: 45\n",
      "Line: 46\n",
      "Line: 47\n",
      "Line: 48\n",
      "Line: 49\n",
      "Line: 50\n",
      "Line: 51\n",
      "Line: 52\n",
      "Line: 53\n",
      "Line: 54\n",
      "Line: 55\n",
      "Line: 56\n",
      "Line: 57\n",
      "Line: 58\n",
      "Line: 59\n",
      "Line: 60\n",
      "Line: 61\n",
      "Line: 62\n",
      "Line: 63\n",
      "Line: 64\n",
      "Line: 65\n",
      "Line: 66\n",
      "Line: 67\n",
      "Line: 68\n",
      "Line: 69\n",
      "Line: 70\n",
      "Line: 71\n",
      "Line: 72\n",
      "Line: 73\n",
      "Line: 74\n",
      "Line: 75\n",
      "Line: 76\n",
      "Line: 77\n",
      "Line: 78\n",
      "Line: 79\n",
      "Line: 80\n",
      "Line: 81\n",
      "Line: 82\n",
      "Line: 83\n",
      "Line: 84\n",
      "Line: 85\n",
      "Line: 86\n",
      "Line: 87\n",
      "Line: 88\n",
      "Line: 89\n",
      "Line: 90\n",
      "Line: 91\n",
      "Line: 92\n",
      "Line: 93\n",
      "Line: 94\n",
      "Line: 95\n",
      "Line: 96\n",
      "Line: 97\n",
      "Line: 98\n",
      "Line: 99\n",
      "Line: 100\n",
      "Line: 101\n",
      "Line: 102\n",
      "Line: 103\n",
      "Line: 104\n",
      "Line: 105\n",
      "Line: 106\n",
      "Line: 107\n",
      "Line: 108\n",
      "Line: 109\n",
      "Line: 110\n",
      "Line: 111\n",
      "Line: 112\n",
      "Line: 113\n",
      "Line: 114\n",
      "Line: 115\n",
      "Line: 116\n",
      "Line: 117\n",
      "Line: 118\n",
      "Line: 119\n",
      "Line: 120\n",
      "Line: 121\n",
      "Line: 122\n",
      "Line: 123\n",
      "Line: 124\n",
      "Line: 125\n",
      "Line: 126\n",
      "Line: 127\n",
      "Line: 128\n",
      "Line: 129\n",
      "Line: 130\n",
      "Line: 131\n",
      "Line: 132\n",
      "Line: 133\n",
      "Line: 134\n",
      "Line: 135\n",
      "Line: 136\n",
      "Line: 137\n",
      "Line: 138\n",
      "Line: 139\n",
      "Line: 140\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "animal_info(adoption_dict, 0, len(adoption_dict)+1, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start checking at size, check if it is Status = Adopted, if it is replace with NAn \n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 17 fields in line 10, saw 18\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cat_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat_information2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m cat_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m         nrows\n\u001b[1;32m   1706\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 17 fields in line 10, saw 18\n"
     ]
    }
   ],
   "source": [
    "cat_df = pd.read_csv('cat_information2.csv')\n",
    "\n",
    "cat_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
